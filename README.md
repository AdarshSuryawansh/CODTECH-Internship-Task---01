# 🚀 Big Data Analysis with PySpark - Task 01

## 📌 Overview

This project is part of an internship assignment focused on handling and analyzing large-scale datasets using distributed computing tools like **PySpark**. The objective is to showcase the ability to process and extract insights from big data efficiently and scalably.

## 🔍 Project Highlights

- ✔️ Utilized **Apache Spark (PySpark)** for scalable data analysis  
- ✔️ Performed **Exploratory Data Analysis (EDA)** on a large dataset  
- ✔️ Executed data cleaning, transformation, and aggregation operations  
- ✔️ Extracted meaningful insights and trends from the dataset  
- ✔️ Implemented Spark DataFrame API for efficient computation  

## 🛠️ Tools & Technologies

- **Language:** Python  
- **Framework:** PySpark  
- **Platform:** Jupyter Notebook  
- **Libraries:** pandas, matplotlib (optional if used)

## 📁 Files Included

- `Task-01.ipynb` – The main notebook containing the complete PySpark-based analysis.

## 📊 Key Analysis Performed

- Data loading using Spark  
- Schema inference and column-wise inspection  
- Handling missing/null values  
- Descriptive statistics  
- Grouped aggregations  
- Trend analysis and insights generation  

## ✅ How to Run

1. Install [Apache Spark](https://spark.apache.org/downloads.html) and ensure it's properly set up with PySpark.
2. Clone this repository:
   ```bash
   git clone https://github.com/your-username/your-repo-name.git
   cd your-repo-name
